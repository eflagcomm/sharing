本文引导新手如何使用与调试由 ZooKeeper/Hadoop/HBase/Kafka/Spark 构建的高可用集群. [关于高可用的系统](http://coolshell.cn/articles/17459.html)详细介绍了何为高可用, 简单地讲, 就是系统要避免单点故障, 降低故障率.

集群环境的搭建请参考[高可用大数据环境搭建](hadoop-spark-hbase-kafka环境搭建.md), 以下简称为**安装文档**.

务必请注意, 集群中的所有服务器的 hostname 必须加到/ etc/hosts 文件中, 并在所有服务器中共享该文件副本.

## ZooKeeper
ZooKeeper 是高可用平台的核心, 解决分布式集群中应用系统的一致性问题. 大数据平台上需要实现高可用的应用系统都是通过 ZooKeeper 选举出主从节点, 实现服务的高可用. ZooKeeper 本身需要配置在3台或5台服务器上, 通过 [Zab 协议](https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zab+in+words)选举出主从节点, 再为其它应用系统提供选举主从服务.

假定 ZooKeeper 安装在 /opt/zk 目录下. 配置文件位于安装包的 conf 目录下.  ZooKeeper 集群的服务器hostname 是 zk125, zk126, zk127.

#### 本地文档
- 在浏览器中用`file:///opt/zk/docs/index.html`访问本地文档.

#### 配置选项
- ZooKeeper 的全部配置参数在`file:///opt/zk/docs/zookeeperAdmin.html#sc_configuration`部分有详细的描述, 可用的简化配置请参考安装文档中的 ZooKeeper 部分.
- 高性能的配置, 建议把`dataDir`与`dataLogDir`分别放到不同的设备上, 因为把日志与 snapshots 写在相同设备上, 会影响服务的响应时间.
- 通过配置`autopurge.snapRetainCount`与`autopurge.purge.purgeInterval`, 触发 ZooKeeper 自动删除旧的 snapshots 与 logs 文件.
- 每个 ZooKeeper 服务器用唯一的**1-255**的正整数表示, 把该数写在`${dataDir}/myid`文件中, `echo ${unique_number} > ${dataDir}/myid`.
- 调整 JVM max heap size, 避免 ZooKeeper 被交换到磁盘, 导致性能下降. 确保 ZooKeeper 使用的内存不会超过系统的实际可用内存大小.

#### 使用
- 启动服务`bin/zkServer.sh start`, 查看运行状态`bin/zkServer.sh status`(获取主从信息)
- 查看 transaction 日志, `java -cp /opt/zk/lib/slf4j-api-1.6.1.jar:/opt/zk/zookeeper-3.4.8.jar org.apache.zookeeper.server.LogFormatter ${dataLogDir}/version-2/log.xxxxxxxx`
- `echo ${four_letter_command} | nc localhost 2181`, 可用命令包括`conf|cons|crst|dump|envi|ruok|srst|srvr|stat|wchs|wchc|wchp|mntr|`, 参考`docs/zookeeperAdmin.html#sc_zkCommands`了解各个命令的具体意图, 或者直接试用. 重点关注`cons|srvr|stat`.
- 通过运行`bash -x bin/zkServer.sh start`, 发现：
    - java 环境变量应该存放在 conf/java.env 文件中;
    - ZooKeeper 环境变量存放在 zkEnv.sh 文件中;
    - ZooKeeper 连接日志存放在 /opt/zk/zookeeper.out 文件中, 在脚本中指定为`> ./zookeeper.out 2>&1 </dev/null`;
    - ZooKeeper 启动参数为`-Dzookeeper.log.dir=.`, 但是我没有在 /opt/zk 目录发现任何日志文件(TODO)

#### 排错
- 检查配置项是否符合要求.
- 由于文件损坏导致 ZooKeeper 服务无法启动, 并在启动服务时报告 IOException 异常. 在这种情况下, 确保其它ZooKeeper服务器正常, 用`stat`检查是否处于健康状态, 现在你可以在崩溃的服务器上删除`${dataDir}/version-2`与`${dataLogDir}/version-2`两个目录下的所有文件, 然后重启该服务器.
- 目前还没有具体的案例分析(TODO)

## Hadoop
当前的 Hadoop 项目包括 Common/HDFS/YARN/MapReduce 四个模块, 其中 Common 是其它模块的基础, 又由于我们使用 Spark 替代 MapReduce 作为并行计算框架, 因此在这一部分我们重点关注 HDFS/YARN 两个模块.

假定 Hadoop 安装在 /opt/hadoop 目录下. 配置文件位于安装包的 etc/hadoop 目录下.

#### 本地文档
- 在浏览器中用`file:///opt/hadoop/share/doc/hadoop/index.html`访问本地文档.

#### 配置选项
- 可用的简化配置请参考安装文档中的 Hadoop 部分. 每个配置项的具体意义参考对应的 xml 文件.
- 四个主要的默认配置文件 core-default.xml/hdfs-default.xml/mapred-default.xml/yarn-default.xml 在本地浏览器上无法展示, 在文本编辑器中打开. 如果想看表格形式, 请参考在线文档.
    - Common 组件的配置文件`/opt/hadoopt/share/doc/hadoop/hadoop-project-dist/hadoop-common/core-default.xml`.
    - HDFS 组件的配置文件`/opt/hadoopt/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml`.
    - MapReduce 组件的配置文件`/opt/hadoopt/share/doc/hadoop/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml`.
    - YARN 组件的配置文件`/opt/hadoopt/share/doc/hadoop/hadoop-yarn/hadoop-yarn-common/yarn-default.xml`.
- **HDFS 高可用配置**有两种方式QJM/NFS, 我们选择QJM, 请参考文档中的 *High Availability With QJM* 部分.
- Hadoop 中另外一个涉及高可用的模块是YARN, 请参考文档中的 *ResourceManager HA* 部分.
- 关于高可用的配置项, 重点关注名字中含有 **ha** 的配置项.
- 在 etc/hadoop/slaves 文件中只需要添加所有 **datanode** 的 hostname.
- 在所有 **namenode** 节点的 etc/hadoop/yarn-site.xml 文件中需要做以下特别处理:

    ```xml
    <!-- 所有的资源管理器 id 需要列在所有的 yarn-site.xml 文件中 -->
    <!-- 因此本条配置项不是高可用系统中 namenode 特有的 -->
    <!-- rm1,rm2 不是规定的名称, 也可以写成其它名字, 只要能够表述其意图. 不过 -->
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>
    <!--  当前资源管理器的 id, 是高可用系统中 namenode 特有的配置项 -->
    <property>
        <name>yarn.resourcemanager.ha.id</name>
        <value>rm1</value>
    </property>
    ```
    
- 环境变量设置, 只需要在 etc/hadoop/hadoop-env.sh 中设置`JAVA_HOME`, `export JAVA_HOME=/path/to/your/java/home`.
- 关于 native 库请参考本地文档的 *Native Libraries Guide* 部分, 再做设置.
- **疑问**: 既然没有使用 Hadoop 提供的 MapReduce 框架, 为什么还要配置 mapred-site.xml 文件?

#### 使用
- 关于 Hadoop 的快速使用指南, 基本概念, 相关原理以及调优, 请参考我之前的文档[大数据生态系统之 Hadoop 系列](大数据生态系统.md#Hadoop系列), 其中的文档链接都是我整理的比较优秀的参考资料.
- 操作 Common 组件, 详细参考本地文档的 *Common* 部分, 包括代理设置, 机架感知, 安全模式, 授权, Tracing 等内容. 
- sbin/start-all.sh 是废弃的命令, 因为把 HDFS namenode 与 YARN resourcemanager 配置在同一台服务器上, 会有性能影响, 所以建议使用`start-dfs.sh`和`start-yarn.sh`分别启动 HDFS 和 YARN.
- 操作 HDFS, 详细参考本地文档的 *HDFS* 部分
    - 启动`sbin/start-dfs.sh`, 停止`sbin/stop-dfs.sh`
    - 格式化 HDFS, `bin/hdfs namenode -format <ClusterName>`, ClusterName 是 hdfs-site.xml 中`dfs.nameservices`配置的值. **注意**格式化 HDFS, 需要先启动 HDFS.
- 操作 YARN , 详细参考本地文档的 *YARN* 部分
    - 启动`sbin/start-yarn.sh`, 停止`sbin/stop-yarn.sh`
    - 再在 resourcemanager 的备份服务器上执行`sbin/start-yarn.sh`以启动 resourcemanager standby 服务.
    - 注意, 考虑性能因数, resourcemanager 与 namenode 最好不要运行在同一台服务器上.
    - kill 提交的 Spark 应用, `bin/yarn application -kill <AppID>`, 在提交 Spark 任务时, 在终端显示的 `application_xxxxxxxxxxxxx_xxxx` 即为 AppID. 也可以通过 YARN 的管理页面获取.
    - YARN 应用程序的日志记录在由 yarn-site.xml 中的`yarn.nodemanager.log-dirs`指定的目录中.
- 确认高可用集群启动成功
    - 验证 HDFS, 用浏览器访问 HDFS namenode 服务上的50070端口, 在 Overview 页面确认 namenode 分别处于 active 或 standby 状态, 再在 Datanodes 页面确认 datanode 全部启动.
    - 验证 YARN, 用浏览器访问 YARN resourcemanager 服务上的8088端口, 在 About 页面确认 resourcemanager 分别处于 active 或 standby 状态, 再在 Nodes 页面确认 nodemanager 全部启动.
- 通过执行`bash -x sbin/start-dfs.sh`, 可以了解到(默认)环境变量信息:
    - 环境变量`HADOOP_PREFIX=/opt/hadoop/`指定安装文件的路径.
    - Hadoop 日志路径, 在 etc/hadoop/hadoop-env.sh 中设置`HADOOP_LOG_DIR`的路径, 否则默认路径是`$HADOOP_PREFIX/logs`.

#### 排错
- 查看 Hadoop 日志, 根据启动过程中的 WARN/ERROR, 修正配置项, 删除日志目录下的文件, 再次重启集群系统.
- 分析应用程序日志, 在每一个 YARN nodemanager 服务器上的`yarn.nodemanager.log-dirs`目录中包含应用程序输出的文件 stdout/stderr.
- 也可以通过 YARN 管理页面的 Applications 页面查看正在运行的任务日志.

## Kafka
我们使用 Kafka 作为集群的消息搜集器. 成百上千的客户端程序作为消息的生产者, 同时向 Kafka 集群提供数据. 部署在集群上的应用程序作为消费者从 Kafka 集群消费数据. 数据可以在 Kafka 集群中保留一段时间, 当消费者应用程序失败重启后, 可以从失败点重新读取数据.

假定 Kafka 安装在 /opt/kafka 目录下. 配置文件位于安装包的 config 目录下.

#### 在线文档
- 由于 Kafka 的本地文档的组织结构没有构成超链接形式, 推荐使用[在线文档](http://kafka.apache.org)

#### 配置选项
- [Broker Configs](http://kafka.apache.org/documentation.html#brokerconfigs)用于启动 Kafka 服务, 可用的简化配置请参考安装文档中的 Kafka 部分.
    - ZooKeeper服务器: zookeeper.connect=zk1:2181,zk2:2181,zk3:2181
    - broker.id, 可以手动为 Kafka 集群上的服务器指定唯一的数字编号, 也可以通过 ZooKeeper 自动生成.
    - log.dir, 设置 Kafka 的 log data, 默认存放在 /tmp/kafka-logs 目录下. 该目录不同于 Kafka 的运行日志目录, 运行日志目录由启动脚本指定, 默认在安装目录的 logs 目录下.
    - log.retenton.bytes log.retention.{hours|minutes|ms}, 设置可以缓存数据的大小或保留时长.
    - socket.request.max.bytes, 设置 Kafka 消费者可以一次性获得的最大数据大小.
    - num.io.threads 与 num.network.threads, 影响程序的读写性能.
- [Consumer Configs](http://kafka.apache.org/documentation#consumerconfigs)用于配置消费者的参数, 如果编写应用程序, 首要关注如下几个参数:
    - 目前只在 Spark 环境使用, 借助 `org.apache.spark.streaming.kafka.KafkaUtils` 提供的接口, 提供 ZooKeeper 的服务器地址列表(zkQurorum), Kafka group id, 以及 Kafka topics 列表.
    - 关于直接使用 Kafka 提供的消费者接口, 待使用后, 再补充.
- [Producer Configs](http://kafka.apache.org/documentation#producerconfigs)用于配置生产者的参数, 在工程的 resources 目录下新建 kafka-producer.properties 文件, 主要配置如下几个键值参数:
    - bootstrap.servers, 指定连接哪些 Kafka Broker 服务器.
    - key.serializer 与 value.serializer, 指定用哪个序列化类操作发送的数据.
    - topic, 指定向哪个 Kafka topic 发送数据.
    - 关于 acks/retries/batch.size/linger.ms/buffer.memory/max.request.size 等几个参数请参考链接文档.
- Kafka 可以配置在几台单独的服务器上, 也可以配置在 HDFS datanode 或 YARN nodemanager 的其中几台worker服务器上. 注意, 不是每一台 worker 服务器上都要配置 Kafka 服务.

#### 使用
- 请使用 ZooKeeper 组件提供的服务, 不使用 Kafka 自带的 Zookeeper 功能.
- 创建, 删除, 修改, 查看 Kafka topics 的方法(ZooKeeper 服务器列表可以只写一个活跃主机):
    - 创建, `bin/kafka-topics.sh --zookeeper zk125,zk126,zk127 --create --replication N --partition N --topic NAME`
    - 删除, `bin/kafka-topics.sh --zookeeper zk125 --delete --topic NAME`, 需要在 Broker 的配置文件中设置`delete.topic.enable=true`
    - 修改, `bin/kafka-topics.sh --zookeeper zk125 --alter --topic NAME --config Key=Value`
    - 查看, `bin/kafka-topics.sh --zookeeper zk125 --describe --topic NAME`, 有时候我们修改了 Broker 的配置文件, 但是配置不起效, 可用该命令查看下具体的配置信息.
- 测试 Kafka topic 是否可用:
    - 生产者, `bin/kafka-console-producer.sh --broker-list BrokerHostname:9092 --topic NAME`, 然后在终端输入任意信息, 看消费者能否收到.
    - 消费者, `bin/kafka-console-consumer.sh --zookeeper zk125 --topic NAME`, 原样显示生产者输入的信息, `--from-beginning`选项指定从 topic 缓存的数据起始处读取信息.

#### 排错
- 检查 Broker 启动的日志是否有异常. 检查连接的 Broker 是否处于活跃状态.
- 检查 Kafka topic 的配置, 是否和要求一致.
- 检查 /opt/kafka/logs 目录下的日志是否有异常.

## HBase
我们用 HBase 存储结果数据, 借助 HBase 提供的数据存储功能, 优化格式化数据的查询操作.

假定 HBase 安装在 /opt/hbase 目录下. 配置文件位于安装包的 conf 目录下.

#### 本地文档
- 在浏览器中用`file:///opt/hbase/docs/index.html`访问本地文档.
 
#### 配置选项
- 可用的简化配置请参考安装文档中的 HBase 部分. 主要有三个配置文件 conf/hbase-env.sh, conf/hbase-site.xml, conf/regionservers, 其次要注意所用的 jdk 的版本.
- 更多信息请参考`file:///opt/hbase/docs/book.html`

#### 使用
- 参考本地文档中的 *The Apache HBase Shell* 部分, 了解如何在命令行下操作 HBase.
- 参考本地文档中的 *Apache HBase APIs* 部分的例子, 了解如何使用 Java 操作 HBase.
- 参考本地文档中的 *HBase and Spark* 部分, 了解在 Spark 中如何使用 HBase. 该功能在 HBase 1.2.2 版本中已经提供, 但是在 maven 中没有相应的依赖文件可用. TODO. 目前的解决办法是使用 Spark 提供的`saveAsNewAPIHadoopDataset`接口处理的, 请参考下面的代码:

    ```java
    // maven依赖 hbase-client 模块
    // 在工程的 resources 目录中添加 hbase-site.xml 与 hdfs-site.xml 两个配置文件
    // 配置文件可以直接使用集群中的配置文件
    javaDStream.foreachRDD(rdd -> {
        Configuration hbaseConfig = HBaseConfiguration.crate();
        hbaseConfig.set(TableOutputFormat.OUTPUT_TABLE, "TableName");

        Job job = Job.getInstance(hbaseConfig);
        job.setOutputKeyClass(ImmutableBytesWritable.class);
        job.setOutputValueClass(Result.class);
        job.setOutputFormatClass(TableOutputFormat.class);
        
        JavaPairRDD pairRdd = rdd.mapToPair(innerRdd -> {
            String uniqueRowKey = "xxxxxx"; // 由 innerRdd 生成
            Put put = new Put(Bytes.toBytes(uniqueRowKey));
            Iterator<> iterator;// 由 innerRdd 生成 Key-Value 迭代器
            while (iterator.hasNext()) {
                String key, value; // 由 iterator 中的内容生成
                put.addColumn(Bytes.toBytes("ColumnFamilyName"), Bytes.toBytes(key), Bytes.toBytes(value));
            }
            return new Tuple2<>(new ImmutableBytesWritable(), put);
        });
        pairRdd.saveAsNewAPIHadoopDataset(job.getConfiguration());
    });
    ```

#### 排错
- 参考本地文档中的 *Troubleshooting and Debugging Apache HBase* 部分, 介绍的非常全面.
- 实践中遇到的问题, TODO

## Spark
相比于 Hadoop 的 MapReduce 计算框架, 基于内存的 Spark 性能优势要突出很多, 鉴于此我们选择 Spark 作为计算框架. 其次 Spark 整合了 Hadoop 生态系统中的多个功能组件, 如 storm/hive/Mahout等, 使大数据平台的易用性得到提高.

假定 Spark 安装在 /opt/spark 目录下. 配置文件位于安装包的 conf 目录下.

#### 在线文档
- 在线文档[索引页](http://spark.apache.org/docs/latest/), 建议对该页进行完全的浏览, 了解 Spark 文档的组织结构.

#### 配置选项
- Spark 的配置与安装都相对简单, 请参考安装文档的 Spark 部分, 主要有三个配置文件需要重点关注 conf/spark-env.sh, conf/spark-defaults.conf, conf/slaves.

#### 使用
- 关于 Spark 的快速使用指南, 基本概念, 相关原理以及调优, 请参考我之前的文档[大数据生态系统之 Spark 系列](大数据生态系统.md#Spark系列), 其中的文档链接都是我整理的比较优秀的参考资料.
- 通过 bin/spark-submit 向集群提交 Spark 应用程序, 请详细参考官方文档 [Submitting Applications](http://spark.apache.org/docs/latest/submitting-applications.html). 此外, 通过下面的例子特别强调下 `--jars` 与 `--conf` 选项:
    
    ```bash
    # --jars 选项用于指定依赖的 jar 文件的路径, 可以是 file:, hdfs:, http:, https, ftp:, local:
    # file: 指定 jar 文件来源于 Spark Driver 的 HTTP 文件服务器, 因此其后指定的 jar 文件必须存在于执行 spark-submit 的服务器上
    # local: 指定 jar 文件存在于 Spark Executor 运行的服务器, 避免下载 jar 文件的网络开销
    # 其它标示符的意义请参考 Submitting Applications 的 Advanced Dependency Management 部分
    # --jars 中的')'后接直接接 file, 是因为前面的'$()'语句会生成一个多余的','
    bin/spark-submit --class org.eflag.spark.main.ExampleMain --master yarn --deploy-mode cluster \
      --jars $(for jar in /opt/hbase/lib/*.jar; do echo local:$jar; done | tr '\n' ',')file:$HOME/spark-streaming-kafka-0-8-assembly_2.11-2.0.0.jar \
      --conf spark.yarn.jars=$(for jar in /opt/spark/jars/*.jar; do echo local:$jar; done | tr '\n' ',') \
      --driver-cores 2 --num-executors 8 --executor-memory 6G --driver-memory 4G \
      $HOME/example-1.0-SNAPSHOT.jar
    ```

- [监视 Spark 应用](http://spark.apache.org/docs/latest/running-on-yarn.html#debugging-your-application):
    - 启动历史日志服务器, `sbin/start-history-server.sh hdfs://eflagcluster/sparkeventlog`, 路径指定了历史日志存放的地方. 如果不带参数(由 conf/spark-defaults.conf 设置 `spark.history.fs.logDirectory hdfs://eflagcluster/sparkeventlog` ).
    - 通过浏览器访问日志服务器的 18080 端口.
    - [解读 Spark 监视页面](http://www.csdn.net/article/2015-07-15/2825214), [英文原文](https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-apache-spark-streaming-applications.html)
    - 还要启动 MapReduce history server, 这样可以访问与 YARN 容器相关日志. 由 yarn-site.xml 中的 `yarn.log.server.url` 指定 nna 为历史日志服务器, 在其 Hadoop 的安装目录执行 `sbin/mr-jobhistory-daemon.sh start historyserver`. 经过这样的设置, 当任务结束时, 日志会被打包上传到 HDFS 中, 可以通过 Spark 历史日志服务访问到结束任务的日志. 如果不做设置, 默认 YARN 会在3小时后删除历史日志.
- API, 使用过程中整理的相关资料, 更多 API 相关资料见官网介绍.
    - [把多个文件读取到一个 RDD](http://stackoverflow.com/questions/24029873/how-to-read-multiple-text-files-into-a-single-rdd)
    - [mapPartitions 与 map 的区别](http://stackoverflow.com/a/39203798/2738837)
    - [RDD/DataFrame/Dataset 区别](http://stackoverflow.com/questions/31508083/difference-between-dataframe-and-rdd-in-spark)
    - [spark streaming 有状态的操作](http://stackoverflow.com/documentation/apache-spark/1924/stateful-operations-in-spark-streaming#t=201609080250582975776)
    - [mapWithState, 比 updateStateByKey 更快地处理带状态的流](https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-apache-spark-streaming.html)
    - [关于 Spark Accumulator 的深入探讨, 主要是缺陷, 避免踩坑](http://imranrashid.com/posts/Spark-Accumulators/)

#### 排错
- 程序使用方面. TODO
- API 方面. TODO

## 大数据系统排错
#### 操作系统
- Ubuntu 16.04 自带的 /bin/kill 程序存在 [Bug](https://bugs.launchpad.net/ubuntu/+source/procps/+bug/1610499), 导致 /bin/yarn 关闭应用程序时, 杀掉所有当前用户的程序. 编译 procps-ng-3.3.12.tar.xz, 用 kill 覆盖 /bin/kill. 

